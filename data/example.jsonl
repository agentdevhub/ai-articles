{"id": "2506.09968v1", "pdf": "http://arxiv.org/pdf/2506.09968v1", "abs": "http://arxiv.org/abs/2506.09968v1", "authors": ["Wentao Ge", "Yuqing Sun", "Ziyan Wang", "Haoyue Zheng", "Weiyang He", "Piaohong Wang", "Qianyu Zhu", "Benyou Wang"], "title": "SRLAgent: Enhancing Self-Regulated Learning Skills through Gamification and LLM Assistance", "categories": ["cs.HC", "I.2.1; I.2.6"], "comment": "14 pages", "summary": "Self-regulated learning (SRL) is crucial for college students navigating\nincreased academic demands and independence. Insufficient SRL skills can lead\nto disorganized study habits, low motivation, and poor time management,\nundermining learners ability to thrive in challenging environments. Through a\nformative study involving 59 college students, we identified key challenges\nstudents face in developing SRL skills, including difficulties with\ngoal-setting, time management, and reflective learning. To address these\nchallenges, we introduce SRLAgent, an LLM-assisted system that fosters SRL\nskills through gamification and adaptive support from large language models\n(LLMs). Grounded in Zimmermans three-phase SRL framework, SRLAgent enables\nstudents to engage in goal-setting, strategy execution, and self-reflection\nwithin an interactive game-based environment. The system offers real-time\nfeedback and scaffolding powered by LLMs to support students independent study\nefforts. We evaluated SRLAgent using a between-subjects design, comparing it to\na baseline system (SRL without Agent features) and a traditional multimedia\nlearning condition. Results showed significant improvements in SRL skills\nwithin the SRLAgent group (p < .001, Cohens d = 0.234) and higher engagement\ncompared to the baselines. This work highlights the value of embedding SRL\nscaffolding and real-time AI support within gamified environments, offering\ndesign implications for educational technologies that aim to promote deeper\nlearning and metacognitive skill development.", "title_zh": "SRLAgent：通过游戏化与大型语言模型辅助提升自主学习能力", "AI": {"task": "研究如何通过SRLAgent系统提升大学生的自我调节学习（SRL）技能", "motivation": "大学生在面临增加的学术要求和独立性时，缺乏SRL技能会导致学习习惯混乱、动机低下和时间管理不善，影响他们在挑战性环境中的表现", "method": "通过一项涉及59名大学生的形成性研究，识别学生在发展SRL技能方面的关键挑战，并引入SRLAgent系统，这是一个基于Zimmerman三阶段SRL框架的LLM辅助系统，通过游戏化和大型语言模型（LLMs）的适应性支持来培养SRL技能", "result": "SRLAgent组在SRL技能上有显著提高（p < .001, Cohen’s d = 0.234），并且与基线系统相比，参与度更高", "conclusion": "在游戏化环境中嵌入SRL支架和实时AI支持，对于旨在促进深度学习和元认知技能发展的教育技术提供了设计启示"}}
{"id": "2506.09965v1", "pdf": "http://arxiv.org/pdf/2506.09965v1", "abs": "http://arxiv.org/abs/2506.09965v1", "authors": ["Junfei Wu", "Jian Guan", "Kaituo Feng", "Qiang Liu", "Shu Wu", "Liang Wang", "Wei Wu", "Tieniu Tan"], "title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing", "categories": ["cs.CV", "cs.AI", "I.2"], "comment": null, "summary": "As textual reasoning with large language models (LLMs) has advanced\nsignificantly, there has been growing interest in enhancing the multimodal\nreasoning capabilities of large vision-language models (LVLMs). However,\nexisting methods primarily approach multimodal reasoning in a straightforward,\ntext-centric manner, where both reasoning and answer derivation are conducted\npurely through text, with the only difference being the presence of multimodal\ninput. As a result, these methods often encounter fundamental limitations in\nspatial reasoning tasks that demand precise geometric understanding and\ncontinuous spatial tracking-capabilities that humans achieve through mental\nvisualization and manipulation. To address the limitations, we propose drawing\nto reason in space, a novel paradigm that enables LVLMs to reason through\nelementary drawing operations in the visual space. By equipping models with\nbasic drawing operations, including annotating bounding boxes and drawing\nauxiliary lines, we empower them to express and analyze spatial relationships\nthrough direct visual manipulation, meanwhile avoiding the performance ceiling\nimposed by specialized perception tools in previous tool-integrated reasoning\napproaches. To cultivate this capability, we develop a three-stage training\nframework: cold-start training with synthetic data to establish basic drawing\nabilities, reflective rejection sampling to enhance self-reflection behaviors,\nand reinforcement learning to directly optimize for target rewards. Extensive\nexperiments demonstrate that our model, named VILASR, consistently outperforms\nexisting methods across diverse spatial reasoning benchmarks, involving maze\nnavigation, static spatial reasoning, video-based reasoning, and\nmulti-view-based reasoning tasks, with an average improvement of 18.4%.", "title_zh": "《通过交织思维与视觉绘图强化视觉语言模型的空间推理能力》", "AI": {"task": "增强大型视觉语言模型（LVLMs）的多模态推理能力", "motivation": "现有的方法在多模态推理上主要采用以文本为中心的简单方式，在需要精确几何理解和连续空间跟踪的空间推理任务中存在根本限制", "method": "提出了一种新颖的范式，通过在视觉空间中进行基本绘图操作来推理，包括标注边界框和绘制辅助线，以及开发了一个三阶段训练框架：使用合成数据进行冷启动训练以建立基本绘图能力，反射拒绝采样以增强自我反思行为，和强化学习以直接优化目标奖励", "result": "我们的模型VILASR在多样化的空间推理基准测试中，包括迷宫导航、静态空间推理、基于视频的推理和基于多视图的推理任务，平均提高了18.4%", "conclusion": "通过在视觉空间中进行基本绘图操作来推理的新范式，能够有效提升大型视觉语言模型在多模态推理任务中的表现"}}
{"id": "2506.09956v1", "pdf": "http://arxiv.org/pdf/2506.09956v1", "abs": "http://arxiv.org/abs/2506.09956v1", "authors": ["Sahar Abdelnabi", "Aideen Fay", "Ahmed Salem", "Egor Zverev", "Kai-Chieh Liao", "Chi-Huang Liu", "Chun-Chih Kuo", "Jannis Weigend", "Danyael Manlangit", "Alex Apostolov", "Haris Umair", "João Donato", "Masayuki Kawakita", "Athar Mahboob", "Tran Huu Bach", "Tsun-Han Chiang", "Myeongjin Cho", "Hajin Choi", "Byeonghyeon Kim", "Hyeonjin Lee", "Benjamin Pannell", "Conor McCauley", "Mark Russinovich", "Andrew Paverd", "Giovanni Cherubin"], "title": "LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection Challenge", "categories": ["cs.CR", "cs.AI"], "comment": "Dataset at:\n  https://huggingface.co/datasets/microsoft/llmail-inject-challenge", "summary": "Indirect Prompt Injection attacks exploit the inherent limitation of Large\nLanguage Models (LLMs) to distinguish between instructions and data in their\ninputs. Despite numerous defense proposals, the systematic evaluation against\nadaptive adversaries remains limited, even when successful attacks can have\nwide security and privacy implications, and many real-world LLM-based\napplications remain vulnerable. We present the results of LLMail-Inject, a\npublic challenge simulating a realistic scenario in which participants\nadaptively attempted to inject malicious instructions into emails in order to\ntrigger unauthorized tool calls in an LLM-based email assistant. The challenge\nspanned multiple defense strategies, LLM architectures, and retrieval\nconfigurations, resulting in a dataset of 208,095 unique attack submissions\nfrom 839 participants. We release the challenge code, the full dataset of\nsubmissions, and our analysis demonstrating how this data can provide new\ninsights into the instruction-data separation problem. We hope this will serve\nas a foundation for future research towards practical structural solutions to\nprompt injection.", "title_zh": "LLMail-Inject：基于真实场景自适应提示注入挑战的数据集", "AI": {"task": "系统评估间接提示注入攻击对大型语言模型（LLMs）的影响及防御策略的有效性", "motivation": "大型语言模型（LLMs）在区分输入中的指令和数据方面存在固有局限，且成功的攻击可能对安全和隐私产生广泛影响，许多基于LLM的实际应用仍处于脆弱状态", "method": "通过LLMail-Inject公共挑战模拟现实场景，参与者尝试在电子邮件中注入恶意指令以触发基于LLM的电子邮件助手中的未授权工具调用，挑战涵盖多种防御策略、LLM架构和检索配置", "result": "收集了来自839名参与者的208,095次独特攻击提交的数据集，发布了挑战代码、完整提交数据集及分析，展示了这些数据如何为解决指令-数据分离问题提供新见解", "conclusion": "希望这项工作能为未来研究提供基础，推动开发实用的结构性解决方案以应对提示注入问题"}}
{"id": "2506.09707v1", "pdf": "http://arxiv.org/pdf/2506.09707v1", "abs": "http://arxiv.org/abs/2506.09707v1", "authors": ["Suhas BN", "Andrew M. Sherrill", "Jyoti Alaparthi", "Dominik Mattioli", "Rosa I. Arriaga", "Chris W. Wiese", "Saeed Abdullah"], "title": "Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements", "categories": ["eess.AS", "cs.CL", "cs.HC", "68T07", "I.2.7; I.5.4; H.5.2"], "comment": "5 pages, 2 figures", "summary": "Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic\nstress disorder (PTSD), but evaluating therapist fidelity remains\nlabor-intensive due to the need for manual review of session recordings. We\npresent a method for the automatic temporal localization of key PE fidelity\nelements -- identifying their start and stop times -- directly from session\naudio and transcripts. Our approach fine-tunes a large pre-trained\naudio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process\nfocused 30-second windows of audio-transcript input. Fidelity labels for three\ncore protocol phases -- therapist orientation (P1), imaginal exposure (P2), and\npost-imaginal processing (P3) -- are generated via LLM-based prompting and\nverified by trained raters. The model is trained to predict normalized boundary\noffsets using soft supervision guided by task-specific prompts. On a dataset of\n313 real PE sessions, our best configuration (LoRA rank 8, 30s windows)\nachieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further\nanalyze the effects of window size and LoRA rank, highlighting the importance\nof context granularity and model adaptation. This work introduces a scalable\nframework for fidelity tracking in PE therapy, with potential to support\nclinician training, supervision, and quality assurance.", "title_zh": "基于LoRA微调大型音频语言模型以实现长时间暴露疗法要素的精准时序定位", "AI": {"task": "自动时间定位关键PE忠诚度元素——从会话音频和转录中直接识别它们的开始和停止时间", "motivation": "由于需要手动审查会话录音，评估治疗师的忠诚度仍然劳动密集", "method": "使用低秩适应（LoRA）对大型预训练音频语言模型Qwen2-Audio进行微调，以处理聚焦的30秒音频转录输入窗口", "result": "在313个真实PE会话的数据集上，我们的最佳配置（LoRA等级8，30秒窗口）在所有任务中实现了5.3秒的平均绝对误差（MAE）", "conclusion": "这项工作为PE治疗中的忠诚度跟踪引入了一个可扩展的框架，有可能支持临床医生的培训、监督和质量保证"}}
{"id": "2506.09696v1", "pdf": "http://arxiv.org/pdf/2506.09696v1", "abs": "http://arxiv.org/abs/2506.09696v1", "authors": ["Joseph Corneli", "Charles J. Danoff", "Raymond S. Puzio", "Sridevi Ayloo", "Serge Belich", "Mary Tedeschi"], "title": "Patterns of Patterns III", "categories": ["cs.HC"], "comment": "18 pages; submitted to Pattern Languages of Programs 2025", "summary": "Building on earlier installments, this paper re-examines the PLACARD pattern.\nWe report on a series of workshops where PLACARD was used to scaffold\ncollaborative reflection, speculative inquiry, and stimulate design pattern\ngeneration. These accounts are enriched by a comparison case: virtual workshops\ncarried out with simple AI-based chatbots. We discuss limitations and lessons\nlearned from both the human and multi-agent settings. We conclude by outlining\na future development strategy at the intersection of AI agents, design\npatterns, and institutional governance.", "title_zh": "模式之模式 III", "AI": {"task": "re-examines the PLACARD pattern", "motivation": "", "method": "a series of workshops where PLACARD was used to scaffold collaborative reflection, speculative inquiry, and stimulate design pattern generation, enriched by a comparison case: virtual workshops carried out with simple AI-based chatbots", "result": "discuss limitations and lessons learned from both the human and multi-agent settings", "conclusion": "outlining a future development strategy at the intersection of AI agents, design patterns, and institutional governance"}}
{"id": "2506.09659v1", "pdf": "http://arxiv.org/pdf/2506.09659v1", "abs": "http://arxiv.org/abs/2506.09659v1", "authors": ["Eltayeb Ahmed", "Uljad Berdica", "Martha Elliott", "Danijela Horak", "Jakob N. Foerster"], "title": "Intent Factored Generation: Unleashing the Diversity in Your Language Model", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Obtaining multiple meaningfully diverse, high quality samples from Large\nLanguage Models for a fixed prompt remains an open challenge. Current methods\nfor increasing diversity often only operate at the token-level, paraphrasing\nthe same response. This is problematic because it leads to poor exploration on\nreasoning problems and to unengaging, repetitive conversational agents. To\naddress this we propose Intent Factored Generation (IFG), factorising the\nsampling process into two stages. First, we sample a semantically dense intent,\ne.g., a summary or keywords. Second, we sample the final response conditioning\non both the original prompt and the intent from the first stage. This allows us\nto use a higher temperature during the intent step to promote conceptual\ndiversity, and a lower temperature during the final generation to ensure the\noutputs are coherent and self-consistent. Additionally, we find that prompting\nthe model to explicitly state its intent for each step of the chain-of-thought\nbefore generating the step is beneficial for reasoning tasks. We demonstrate\nour method's effectiveness across a diverse set of tasks. We show this method\nimproves both pass@k and Reinforcement Learning from Verifier Feedback on maths\nand code tasks. For instruction-tuning, we combine IFG with Direct Preference\nOptimisation to increase conversational diversity without sacrificing reward.\nFinally, we achieve higher diversity while maintaining the quality of\ngenerations on a general language modelling task, using a new dataset of reader\ncomments and news articles that we collect and open-source. In summary, we\npresent a simple method of increasing the sample diversity of LLMs while\nmaintaining performance. This method can be implemented by changing the prompt\nand varying the temperature during generation, making it easy to integrate into\nmany algorithms for gains across various applications.", "title_zh": "意图分解生成：释放语言模型的多样性潜力", "AI": {"task": "Obtaining multiple meaningfully diverse, high quality samples from Large Language Models for a fixed prompt", "motivation": "Current methods for increasing diversity often only operate at the token-level, paraphrasing the same response, leading to poor exploration on reasoning problems and to unengaging, repetitive conversational agents", "method": "Intent Factored Generation (IFG), factorising the sampling process into two stages: sampling a semantically dense intent and then sampling the final response conditioning on both the original prompt and the intent", "result": "Improves both pass@k and Reinforcement Learning from Verifier Feedback on maths and code tasks, increases conversational diversity without sacrificing reward for instruction-tuning, and achieves higher diversity while maintaining the quality of generations on a general language modelling task", "conclusion": "A simple method of increasing the sample diversity of LLMs while maintaining performance, easy to integrate into many algorithms for gains across various applications"}}
{"id": "2506.09632v1", "pdf": "http://arxiv.org/pdf/2506.09632v1", "abs": "http://arxiv.org/abs/2506.09632v1", "authors": ["Eva Paraschou", "Maria Michali", "Sofia Yfantidou", "Stelios Karamanidis", "Stefanos Rafail Kalogeros", "Athena Vakali"], "title": "Ties of Trust: a bowtie model to uncover trustor-trustee relationships in LLMs", "categories": ["cs.CY"], "comment": "Accepted for publication at The 2025 ACM Conference on Fairness,\n  Accountability, and Transparency (FAccT '25). This version corresponds to the\n  camera-ready manuscript submitted to the conference proceedings", "summary": "The rapid and unprecedented dominance of Artificial Intelligence (AI),\nparticularly through Large Language Models (LLMs), has raised critical trust\nchallenges in high-stakes domains like politics. Biased LLMs' decisions and\nmisinformation undermine democratic processes, and existing trust models fail\nto address the intricacies of trust in LLMs. Currently, oversimplified,\none-directional approaches have largely overlooked the many relationships\nbetween trustor (user) contextual factors (e.g. ideology, perceptions) and\ntrustee (LLMs) systemic elements (e.g. scientists, tool's features). In this\nwork, we introduce a bowtie model for holistically conceptualizing and\nformulating trust in LLMs, with a core component comprehensively exploring\ntrust by tying its two sides, namely the trustor and the trustee, as well as\ntheir intricate relationships. We uncover these relationships within the\nproposed bowtie model and beyond to its sociotechnical ecosystem, through a\nmixed-methods explanatory study, that exploits a political discourse analysis\ntool (integrating ChatGPT), by exploring and responding to the next critical\nquestions: 1) How do trustor's contextual factors influence trust-related\nactions? 2) How do these factors influence and interact with trustee systemic\nelements? 3) How does trust itself vary across trustee systemic elements? Our\nbowtie-based explanatory analysis reveals that past experiences and familiarity\nsignificantly shape trustor's trust-related actions; not all trustor contextual\nfactors equally influence trustee systemic elements; and trustee's\nhuman-in-the-loop features enhance trust, while lack of transparency decreases\nit. Finally, this solid evidence is exploited to deliver recommendations,\ninsights and pathways towards building robust trusting ecosystems in LLM-based\nsolutions.", "title_zh": "信任纽带：基于领结模型揭示大语言模型中的信任方与被信任方关系", "AI": {"task": "解决大型语言模型（LLMs）在政治等高风险领域中引发的信任挑战", "motivation": "偏见的LLMs决策和错误信息破坏民主进程，现有信任模型未能解决LLMs信任的复杂性", "method": "引入蝴蝶结模型，通过混合方法解释性研究，探索信任者（用户）和受托者（LLMs）之间的关系及其社会技术生态系统", "result": "过去经验和熟悉度显著影响信任者的信任相关行为；并非所有信任者情境因素均等影响受托者系统元素；受托者的人机交互特征增强信任，而缺乏透明度则降低信任", "conclusion": "利用这些坚实证据提供建议、见解和路径，以构建基于LLM解决方案的强大信任生态系统"}}
{"id": "2506.09420v1", "pdf": "http://arxiv.org/pdf/2506.09420v1", "abs": "http://arxiv.org/abs/2506.09420v1", "authors": ["Henry Peng Zou", "Wei-Chieh Huang", "Yaozu Wu", "Chunyu Miao", "Dongyuan Li", "Aiwei Liu", "Yue Zhou", "Yankai Chen", "Weizhi Zhang", "Yangning Li", "Liancheng Fang", "Renhe Jiang", "Philip S. Yu"], "title": "A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG", "cs.MA"], "comment": null, "summary": "Recent improvements in large language models (LLMs) have led many researchers\nto focus on building fully autonomous AI agents. This position paper questions\nwhether this approach is the right path forward, as these autonomous systems\nstill have problems with reliability, transparency, and understanding the\nactual requirements of human. We suggest a different approach: LLM-based\nHuman-Agent Systems (LLM-HAS), where AI works with humans rather than replacing\nthem. By keeping human involved to provide guidance, answer questions, and\nmaintain control, these systems can be more trustworthy and adaptable. Looking\nat examples from healthcare, finance, and software development, we show how\nhuman-AI teamwork can handle complex tasks better than AI working alone. We\nalso discuss the challenges of building these collaborative systems and offer\npractical solutions. This paper argues that progress in AI should not be\nmeasured by how independent systems become, but by how well they can work with\nhumans. The most promising future for AI is not in systems that take over human\nroles, but in those that enhance human capabilities through meaningful\npartnership.", "title_zh": "《协同智能的呼唤：为何人机协作系统应优先于人工智能自主性》", "AI": {"task": "质疑构建完全自主AI代理的正确性，并提出LLM-based Human-Agent Systems (LLM-HAS)作为替代方案", "motivation": "自主系统在可靠性、透明度和理解人类实际需求方面仍存在问题", "method": "通过分析医疗、金融和软件开发领域的例子，展示人-AI团队合作如何比AI单独工作更好地处理复杂任务，并讨论构建这些协作系统的挑战及实用解决方案", "result": "展示人-AI团队合作在复杂任务处理上的优势，并提出构建协作系统的实用解决方案", "conclusion": "AI的进步不应以系统的独立性来衡量，而应以它们与人类合作的能力来衡量；AI最有前途的未来不在于接管人类角色的系统，而在于通过有意义的伙伴关系增强人类能力的系统"}}
{"id": "2506.09373v1", "pdf": "http://arxiv.org/pdf/2506.09373v1", "abs": "http://arxiv.org/abs/2506.09373v1", "authors": ["Jiaqi Tang", "Yu Xia", "Yi-Feng Wu", "Yuwei Hu", "Yuhui Chen", "Qing-Guo Chen", "Xiaogang Xu", "Xiangyu Wu", "Hao Lu", "Yanqing Ma", "Shiyin Lu", "Qifeng Chen"], "title": "LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "The advent of autonomous agents is transforming interactions with Graphical\nUser Interfaces (GUIs) by employing natural language as a powerful\nintermediary. Despite the predominance of Supervised Fine-Tuning (SFT) methods\nin current GUI agents for achieving spatial localization, these methods face\nsubstantial challenges due to their limited capacity to accurately perceive\npositional data. Existing strategies, such as reinforcement learning, often\nfail to assess positional accuracy effectively, thereby restricting their\nutility. In response, we introduce Location Preference Optimization (LPO), a\nnovel approach that leverages locational data to optimize interaction\npreferences. LPO uses information entropy to predict interaction positions by\nfocusing on zones rich in information. Besides, it further introduces a dynamic\nlocation reward function based on physical distance, reflecting the varying\nimportance of interaction positions. Supported by Group Relative Preference\nOptimization (GRPO), LPO facilitates an extensive exploration of GUI\nenvironments and significantly enhances interaction precision. Comprehensive\nexperiments demonstrate LPO's superior performance, achieving SOTA results\nacross both offline benchmarks and real-world online evaluations. Our code will\nbe made publicly available soon, at https://github.com/AIDC-AI/LPO.", "title_zh": "LPO：基于位置偏好优化的精准GUI智能体交互", "AI": {"task": "提升自主代理与图形用户界面(GUIs)交互的精确性", "motivation": "当前GUI代理中占主导地位的监督微调(SFT)方法在准确感知位置数据方面能力有限，现有策略如强化学习往往无法有效评估位置准确性，限制了其实用性", "method": "引入位置偏好优化(LPO)方法，利用位置数据优化交互偏好，通过信息熵预测信息丰富区域的交互位置，并引入基于物理距离的动态位置奖励函数，反映交互位置的不同重要性", "result": "综合实验表明LPO方法表现出卓越的性能，在离线基准测试和真实世界在线评估中均达到了最先进(SOTA)的结果", "conclusion": "LPO方法通过GRPO支持，促进了GUI环境的广泛探索，显著提高了交互精确性"}}
{"id": "2506.09354v1", "pdf": "http://arxiv.org/pdf/2506.09354v1", "abs": "http://arxiv.org/abs/2506.09354v1", "authors": ["Kellie Yu Hui Sim", "Roy Ka-Wei Lee", "Kenny Tsu Wei Choo"], "title": "\"Is This Really a Human Peer Supporter?\": Misalignments Between Peer Supporters and Experts in LLM-Supported Interactions", "categories": ["cs.HC", "cs.AI", "H.5.0"], "comment": null, "summary": "Mental health is a growing global concern, prompting interest in AI-driven\nsolutions to expand access to psychosocial support. Peer support, grounded in\nlived experience, offers a valuable complement to professional care. However,\nvariability in training, effectiveness, and definitions raises concerns about\nquality, consistency, and safety. Large Language Models (LLMs) present new\nopportunities to enhance peer support interactions, particularly in real-time,\ntext-based interactions. We present and evaluate an AI-supported system with an\nLLM-simulated distressed client, context-sensitive LLM-generated suggestions,\nand real-time emotion visualisations. 2 mixed-methods studies with 12 peer\nsupporters and 5 mental health professionals (i.e., experts) examined the\nsystem's effectiveness and implications for practice. Both groups recognised\nits potential to enhance training and improve interaction quality. However, we\nfound a key tension emerged: while peer supporters engaged meaningfully,\nexperts consistently flagged critical issues in peer supporter responses, such\nas missed distress cues and premature advice-giving. This misalignment\nhighlights potential limitations in current peer support training, especially\nin emotionally charged contexts where safety and fidelity to best practices are\nessential. Our findings underscore the need for standardised, psychologically\ngrounded training, especially as peer support scales globally. They also\ndemonstrate how LLM-supported systems can scaffold this development--if\ndesigned with care and guided by expert oversight. This work contributes to\nemerging conversations on responsible AI integration in mental health and the\nevolving role of LLMs in augmenting peer-delivered care.", "title_zh": "\"这真的是人类同伴支持者吗？\"：LLM辅助互动中同伴支持者与专家的认知偏差", "AI": {"task": "AI-driven solutions to expand access to psychosocial support", "motivation": "Mental health is a growing global concern, and variability in training, effectiveness, and definitions raises concerns about quality, consistency, and safety in peer support", "method": "Present and evaluate an AI-supported system with an LLM-simulated distressed client, context-sensitive LLM-generated suggestions, and real-time emotion visualisations through 2 mixed-methods studies with 12 peer supporters and 5 mental health professionals", "result": "Both groups recognised its potential to enhance training and improve interaction quality, but a key tension emerged with experts consistently flagging critical issues in peer supporter responses", "conclusion": "There is a need for standardised, psychologically grounded training in peer support, and LLM-supported systems can scaffold this development if designed with care and guided by expert oversight"}}
