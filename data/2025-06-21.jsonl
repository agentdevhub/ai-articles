{"id": "2506.15677v1", "pdf": "http://arxiv.org/pdf/2506.15677v1", "abs": "http://arxiv.org/abs/2506.15677v1", "authors": ["Yining Hong", "Rui Sun", "Bingxuan Li", "Xingcheng Yao", "Maxine Wu", "Alexander Chien", "Da Yin", "Ying Nian Wu", "Zhecan James Wang", "Kai-Wei Chang"], "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MM", "cs.RO"], "comment": null, "summary": "AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/."}
{"id": "2506.15672v1", "pdf": "http://arxiv.org/pdf/2506.15672v1", "abs": "http://arxiv.org/abs/2506.15672v1", "authors": ["Yao Zhang", "Chenyang Lin", "Shijie Tang", "Haokun Chen", "Shijie Zhou", "Yunpu Ma", "Volker Tresp"], "title": "SwarmAgentic: Towards Fully Automated Agentic System Generation via Swarm Intelligence", "categories": ["cs.AI", "cs.MA"], "comment": "41 pages", "summary": "The rapid progress of Large Language Models has advanced agentic systems in\ndecision-making, coordination, and task execution. Yet, existing agentic system\ngeneration frameworks lack full autonomy, missing from-scratch agent\ngeneration, self-optimizing agent functionality, and collaboration, limiting\nadaptability and scalability. We propose SwarmAgentic, a framework for fully\nautomated agentic system generation that constructs agentic systems from\nscratch and jointly optimizes agent functionality and collaboration as\ninterdependent components through language-driven exploration. To enable\nefficient search over system-level structures, SwarmAgentic maintains a\npopulation of candidate systems and evolves them via feedback-guided updates,\ndrawing inspiration from Particle Swarm Optimization (PSO). We evaluate our\nmethod on six real-world, open-ended, and exploratory tasks involving\nhigh-level planning, system-level coordination, and creative reasoning. Given\nonly a task description and an objective function, SwarmAgentic outperforms all\nbaselines, achieving a +261.8% relative improvement over ADAS on the\nTravelPlanner benchmark, highlighting the effectiveness of full automation in\nstructurally unconstrained tasks. This framework marks a significant step\ntoward scalable and autonomous agentic system design, bridging swarm\nintelligence with fully automated system multi-agent generation. Our code is\npublicly released at https://yaoz720.github.io/SwarmAgentic/."}
{"id": "2506.14683v1", "pdf": "http://arxiv.org/pdf/2506.14683v1", "abs": "http://arxiv.org/abs/2506.14683v1", "authors": ["Leonhard Applis", "Yuntong Zhang", "Shanchao Liang", "Nan Jiang", "Lin Tan", "Abhik Roychoudhury"], "title": "Unified Software Engineering agent as AI Software Engineer", "categories": ["cs.SE", "cs.AI"], "comment": "Leonhard Applis and Yuntong Zhang contributed equally to this work", "summary": "The growth of Large Language Model (LLM) technology has raised expectations\nfor automated coding. However, software engineering is more than coding and is\nconcerned with activities including maintenance and evolution of a project. In\nthis context, the concept of LLM agents has gained traction, which utilize LLMs\nas reasoning engines to invoke external tools autonomously. But is an LLM agent\nthe same as an AI software engineer? In this paper, we seek to understand this\nquestion by developing a Unified Software Engineering agent or USEagent. Unlike\nexisting work which builds specialized agents for specific software tasks such\nas testing, debugging, and repair, our goal is to build a unified agent which\ncan orchestrate and handle multiple capabilities. This gives the agent the\npromise of handling complex scenarios in software development such as fixing an\nincomplete patch, adding new features, or taking over code written by others.\nWe envision USEagent as the first draft of a future AI Software Engineer which\ncan be a team member in future software development teams involving both AI and\nhumans. To evaluate the efficacy of USEagent, we build a Unified Software\nEngineering bench (USEbench) comprising of myriad tasks such as coding,\ntesting, and patching. USEbench is a judicious mixture of tasks from existing\nbenchmarks such as SWE-bench, SWT-bench, and REPOCOD. In an evaluation on\nUSEbench consisting of 1,271 repository-level software engineering tasks,\nUSEagent shows improved efficacy compared to existing general agents such as\nOpenHands CodeActAgent. There exist gaps in the capabilities of USEagent for\ncertain coding tasks, which provides hints on further developing the AI\nSoftware Engineer of the future."}
{"id": "2506.14670v1", "pdf": "http://arxiv.org/pdf/2506.14670v1", "abs": "http://arxiv.org/abs/2506.14670v1", "authors": ["Jina Kim", "Leeje Jang", "Yao-Yi Chiang", "Guanyu Wang", "Michelle Pasco"], "title": "StreetLens: Enabling Human-Centered AI Agents for Neighborhood Assessment from Street View Imagery", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Traditionally, neighborhood studies have employed interviews, surveys, and\nmanual image annotation guided by detailed protocols to identify environmental\ncharacteristics, including physical disorder, decay, street safety, and\nsociocultural symbols, and to examine their impact on developmental and health\noutcomes. While these methods yield rich insights, they are time-consuming and\nrequire intensive expert intervention. Recent technological advances, including\nvision-language models (VLMs), have begun to automate parts of this process;\nhowever, existing efforts are often ad hoc and lack adaptability across\nresearch designs and geographic contexts. In this demo paper, we present\nStreetLens, a human-centered, researcher-configurable workflow that embeds\nrelevant social science expertise in a VLM for scalable neighborhood\nenvironmental assessments. StreetLens mimics the process of trained human\ncoders by grounding the analysis in questions derived from established\ninterview protocols, retrieving relevant street view imagery (SVI), and\ngenerating a wide spectrum of semantic annotations from objective features\n(e.g., the number of cars) to subjective perceptions (e.g., the sense of\ndisorder in an image). By enabling researchers to define the VLM's role through\ndomain-informed prompting, StreetLens places domain knowledge at the core of\nthe analysis process. It also supports the integration of prior survey data to\nenhance robustness and expand the range of characteristics assessed across\ndiverse settings. We provide a Google Colab notebook to make StreetLens\naccessible and extensible for researchers working with public or custom SVI\ndatasets. StreetLens represents a shift toward flexible, agentic AI systems\nthat work closely with researchers to accelerate and scale neighborhood\nstudies."}
{"id": "2506.14539v1", "pdf": "http://arxiv.org/pdf/2506.14539v1", "abs": "http://arxiv.org/abs/2506.14539v1", "authors": ["Daewon Kang", "YeongHwan Shin", "Doyeon Kim", "Kyu-Hwan Jung", "Meong Hi Son"], "title": "Doppelgänger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack", "categories": ["cs.AI", "cs.CR"], "comment": null, "summary": "Since the advent of large language models, prompt engineering now enables the\nrapid, low-effort creation of diverse autonomous agents that are already in\nwidespread use. Yet this convenience raises urgent concerns about the safety,\nrobustness, and behavioral consistency of the underlying prompts, along with\nthe pressing challenge of preventing those prompts from being exposed to user's\nattempts. In this paper, we propose the ''Doppelg\\\"anger method'' to\ndemonstrate the risk of an agent being hijacked, thereby exposing system\ninstructions and internal information. Next, we define the ''Prompt Alignment\nCollapse under Adversarial Transfer (PACAT)'' level to evaluate the\nvulnerability to this adversarial transfer attack. We also propose a ''Caution\nfor Adversarial Transfer (CAT)'' prompt to counter the Doppelg\\\"anger method.\nThe experimental results demonstrate that the Doppelg\\\"anger method can\ncompromise the agent's consistency and expose its internal information. In\ncontrast, CAT prompts enable effective defense against this adversarial attack."}
{"id": "2506.14142v1", "pdf": "http://arxiv.org/pdf/2506.14142v1", "abs": "http://arxiv.org/abs/2506.14142v1", "authors": ["Wenting Chen", "Yi Dong", "Zhaojun Ding", "Yucheng Shi", "Yifan Zhou", "Fang Zeng", "Yijun Luo", "Tianyu Lin", "Yihang Su", "Yichen Wu", "Kai Zhang", "Zhen Xiang", "Tianming Liu", "Ninghao Liu", "Lichao Sun", "Yixuan Yuan", "Xiang Li"], "title": "RadFabric: Agentic AI System with Reasoning Capability for Radiology", "categories": ["cs.CV", "cs.CL"], "comment": "4 figures, 2 tables", "summary": "Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic\nconditions, but current automated systems face limitations in pathology\ncoverage, diagnostic accuracy, and integration of visual and textual reasoning.\nTo address these gaps, we propose RadFabric, a multi agent, multimodal\nreasoning framework that unifies visual and textual analysis for comprehensive\nCXR interpretation. RadFabric is built on the Model Context Protocol (MCP),\nenabling modularity, interoperability, and scalability for seamless integration\nof new diagnostic agents. The system employs specialized CXR agents for\npathology detection, an Anatomical Interpretation Agent to map visual findings\nto precise anatomical structures, and a Reasoning Agent powered by large\nmultimodal reasoning models to synthesize visual, anatomical, and clinical data\ninto transparent and evidence based diagnoses. RadFabric achieves significant\nperformance improvements, with near-perfect detection of challenging\npathologies like fractures (1.000 accuracy) and superior overall diagnostic\naccuracy (0.799) compared to traditional systems (0.229 to 0.527). By\nintegrating cross modal feature alignment and preference-driven reasoning,\nRadFabric advances AI-driven radiology toward transparent, anatomically\nprecise, and clinically actionable CXR analysis."}
{"id": "2506.13905v1", "pdf": "http://arxiv.org/pdf/2506.13905v1", "abs": "http://arxiv.org/abs/2506.13905v1", "authors": ["Zhongzhi Yu", "Mingjie Liu", "Michael Zimmer", "Yingyan Celine Lin", "Yong Liu", "Haoxing Ren"], "title": "Spec2RTL-Agent: Automated Hardware Code Generation from Complex Specifications Using LLM Agent Systems", "categories": ["cs.AR"], "comment": null, "summary": "Despite recent progress in generating hardware RTL code with LLMs, existing\nsolutions still suffer from a substantial gap between practical application\nscenarios and the requirements of real-world RTL code development. Prior\napproaches either focus on overly simplified hardware descriptions or depend on\nextensive human guidance to process complex specifications, limiting their\nscalability and automation potential. In this paper, we address this gap by\nproposing an LLM agent system, termed Spec2RTL-Agent, designed to directly\nprocess complex specification documentation and generate corresponding RTL code\nimplementations, advancing LLM-based RTL code generation toward more realistic\napplication settings. To achieve this goal, Spec2RTL-Agent introduces a novel\nmulti-agent collaboration framework that integrates three key enablers: (1) a\nreasoning and understanding module that translates specifications into\nstructured, step-by-step implementation plans; (2) a progressive coding and\nprompt optimization module that iteratively refines the code across multiple\nrepresentations to enhance correctness and synthesisability for RTL conversion;\nand (3) an adaptive reflection module that identifies and traces the source of\nerrors during generation, ensuring a more robust code generation flow. Instead\nof directly generating RTL from natural language, our system strategically\ngenerates synthesizable C++ code, which is then optimized for HLS. This\nagent-driven refinement ensures greater correctness and compatibility compared\nto naive direct RTL generation approaches. We evaluate Spec2RTL-Agent on three\nspecification documents, showing it generates accurate RTL code with up to 75%\nfewer human interventions than existing methods. This highlights its role as\nthe first fully automated multi-agent system for RTL generation from\nunstructured specs, reducing reliance on human effort in hardware design."}
{"id": "2506.13474v1", "pdf": "http://arxiv.org/pdf/2506.13474v1", "abs": "http://arxiv.org/abs/2506.13474v1", "authors": ["David Bani-Harouni", "Chantal Pellegrini", "Ege Özsoy", "Matthias Keicher", "Nassir Navab"], "title": "Language Agents for Hypothesis-driven Clinical Decision Making with Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Clinical decision-making is a dynamic, interactive, and cyclic process where\ndoctors have to repeatedly decide on which clinical action to perform and\nconsider newly uncovered information for diagnosis and treatment. Large\nLanguage Models (LLMs) have the potential to support clinicians in this\nprocess, however, most applications of LLMs in clinical decision support suffer\nfrom one of two limitations: Either they assume the unrealistic scenario of\nimmediate availability of all patient information and do not model the\ninteractive and iterative investigation process, or they restrict themselves to\nthe limited \"out-of-the-box\" capabilities of large pre-trained models without\nperforming task-specific training. In contrast to this, we propose to model\nclinical decision-making for diagnosis with a hypothesis-driven\nuncertainty-aware language agent, LA-CDM, that converges towards a diagnosis\nvia repeatedly requesting and interpreting relevant tests. Using a hybrid\ntraining paradigm combining supervised and reinforcement learning, we train\nLA-CDM with three objectives targeting critical aspects of clinical\ndecision-making: accurate hypothesis generation, hypothesis uncertainty\nestimation, and efficient decision-making. We evaluate our methodology on\nMIMIC-CDM, a real-world dataset covering four abdominal diseases containing\nvarious clinical tests and show the benefit of explicitly training clinical\ndecision-making for increasing diagnostic performance and efficiency."}
{"id": "2506.13171v1", "pdf": "http://arxiv.org/pdf/2506.13171v1", "abs": "http://arxiv.org/abs/2506.13171v1", "authors": ["Lukasz Mazur", "Nenad Petrovic", "James Pontes Miranda", "Ansgar Radermacher", "Robert Rasche", "Alois Knoll"], "title": "Querying Large Automotive Software Models: Agentic vs. Direct LLM Approaches", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) offer new opportunities for interacting with\ncomplex software artifacts, such as software models, through natural language.\nThey present especially promising benefits for large software models that are\ndifficult to grasp in their entirety, making traditional interaction and\nanalysis approaches challenging. This paper investigates two approaches for\nleveraging LLMs to answer questions over software models: direct prompting,\nwhere the whole software model is provided in the context, and an agentic\napproach combining LLM-based agents with general-purpose file access tools. We\nevaluate these approaches using an Ecore metamodel designed for timing analysis\nand software optimization in automotive and embedded domains. Our findings show\nthat while the agentic approach achieves accuracy comparable to direct\nprompting, it is significantly more efficient in terms of token usage. This\nefficiency makes the agentic approach particularly suitable for the automotive\nindustry, where the large size of software models makes direct prompting\ninfeasible, establishing LLM agents as not just a practical alternative but the\nonly viable solution. Notably, the evaluation was conducted using small LLMs,\nwhich are more feasible to be executed locally - an essential advantage for\nmeeting strict requirements around privacy, intellectual property protection,\nand regulatory compliance. Future work will investigate software models in\ndiverse formats, explore more complex agent architectures, and extend agentic\nworkflows to support not only querying but also modification of software\nmodels."}
{"id": "2506.12666v1", "pdf": "http://arxiv.org/pdf/2506.12666v1", "abs": "http://arxiv.org/abs/2506.12666v1", "authors": ["Hitesh Goel", "Hao Zhu"], "title": "LIFELONG SOTOPIA: Evaluating Social Intelligence of Language Agents Over Lifelong Social Interactions", "categories": ["cs.AI"], "comment": null, "summary": "Humans engage in lifelong social interactions through interacting with\ndifferent people under different scenarios for different social goals. This\nrequires social intelligence to gather information through a long time span and\nuse it to navigate various social contexts effectively. Whether AI systems are\nalso capable of this is understudied in the existing research. In this paper,\nwe present a novel benchmark, LIFELONG-SOTOPIA, to perform a comprehensive\nevaluation of language agents by simulating multi-episode interactions. In each\nepisode, the language agents role-play characters to achieve their respective\nsocial goals in randomly sampled social tasks. With LIFELONG-SOTOPIA, we find\nthat goal achievement and believability of all of the language models that we\ntest decline through the whole interaction. Although using an advanced memory\nmethod improves the agents' performance, the best agents still achieve a\nsignificantly lower goal completion rate than humans on scenarios requiring an\nexplicit understanding of interaction history. These findings show that we can\nuse LIFELONG-SOTOPIA to evaluate the social intelligence of language agents\nover lifelong social interactions."}
{"id": "2506.12636v1", "pdf": "http://arxiv.org/pdf/2506.12636v1", "abs": "http://arxiv.org/abs/2506.12636v1", "authors": ["Julia Santaniello", "Matthew Russell", "Benson Jiang", "Donatello Sassaroli", "Robert Jacob", "Jivko Sinapov"], "title": "Mapping Neural Signals to Agent Performance, A Step Towards Reinforcement Learning from Neural Feedback", "categories": ["cs.LG"], "comment": null, "summary": "Implicit Human-in-the-Loop Reinforcement Learning (HITL-RL) is a methodology\nthat integrates passive human feedback into autonomous agent training while\nminimizing human workload. However, existing methods often rely on active\ninstruction, requiring participants to teach an agent through unnatural\nexpression or gesture. We introduce NEURO-LOOP, an implicit feedback framework\nthat utilizes the intrinsic human reward system to drive human-agent\ninteraction. This work demonstrates the feasibility of a critical first step in\nthe NEURO-LOOP framework: mapping brain signals to agent performance. Using\nfunctional near-infrared spectroscopy (fNIRS), we design a dataset to enable\nfuture research using passive Brain-Computer Interfaces for Human-in-the-Loop\nReinforcement Learning. Participants are instructed to observe or guide a\nreinforcement learning agent in its environment while signals from the\nprefrontal cortex are collected. We conclude that a relationship between fNIRS\ndata and agent performance exists using classical machine learning techniques.\nFinally, we highlight the potential that neural interfaces may offer to future\napplications of human-agent interaction, assistive AI, and adaptive autonomous\nsystems."}
