{"id": "2506.15677v1", "pdf": "http://arxiv.org/pdf/2506.15677v1", "abs": "http://arxiv.org/abs/2506.15677v1", "authors": ["Yining Hong", "Rui Sun", "Bingxuan Li", "Xingcheng Yao", "Maxine Wu", "Alexander Chien", "Da Yin", "Ying Nian Wu", "Zhecan James Wang", "Kai-Wei Chang"], "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MM", "cs.RO"], "comment": null, "summary": "AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/.", "title_zh": "具身网络智能体：融合物理与数字领域的智能体集成", "AI": {"task": "开发能够流畅桥接实体化和网络规模推理的AI代理新范式", "motivation": "当前AI代理大多孤立，要么检索和推理在线获取的大量数字信息和知识，要么通过实体感知、规划和行动与物理世界互动，但很少同时进行两者，这限制了它们解决需要整合物理和数字智能的任务的能力", "method": "首先开发了Embodied Web Agents任务环境，一个统一的模拟平台，将现实的3D室内外环境与功能性网络接口紧密集成；在此基础上，构建并发布了Embodied Web Agents Benchmark，包含多样化的任务套件，如烹饪、导航、购物、旅游和地理定位，所有这些任务都需要跨物理和数字领域的协调推理，以系统评估跨领域智能", "result": "实验结果显示，最先进的AI系统与人类能力之间存在显著的性能差距，为实体认知和网络规模知识访问的交叉领域既提出了挑战，也提供了机会", "conclusion": "所有数据集、代码和网站都在我们的项目页面https://embodied-web-agent.github.io/上公开可用"}}
{"id": "2506.15672v1", "pdf": "http://arxiv.org/pdf/2506.15672v1", "abs": "http://arxiv.org/abs/2506.15672v1", "authors": ["Yao Zhang", "Chenyang Lin", "Shijie Tang", "Haokun Chen", "Shijie Zhou", "Yunpu Ma", "Volker Tresp"], "title": "SwarmAgentic: Towards Fully Automated Agentic System Generation via Swarm Intelligence", "categories": ["cs.AI", "cs.MA"], "comment": "41 pages", "summary": "The rapid progress of Large Language Models has advanced agentic systems in\ndecision-making, coordination, and task execution. Yet, existing agentic system\ngeneration frameworks lack full autonomy, missing from-scratch agent\ngeneration, self-optimizing agent functionality, and collaboration, limiting\nadaptability and scalability. We propose SwarmAgentic, a framework for fully\nautomated agentic system generation that constructs agentic systems from\nscratch and jointly optimizes agent functionality and collaboration as\ninterdependent components through language-driven exploration. To enable\nefficient search over system-level structures, SwarmAgentic maintains a\npopulation of candidate systems and evolves them via feedback-guided updates,\ndrawing inspiration from Particle Swarm Optimization (PSO). We evaluate our\nmethod on six real-world, open-ended, and exploratory tasks involving\nhigh-level planning, system-level coordination, and creative reasoning. Given\nonly a task description and an objective function, SwarmAgentic outperforms all\nbaselines, achieving a +261.8% relative improvement over ADAS on the\nTravelPlanner benchmark, highlighting the effectiveness of full automation in\nstructurally unconstrained tasks. This framework marks a significant step\ntoward scalable and autonomous agentic system design, bridging swarm\nintelligence with fully automated system multi-agent generation. Our code is\npublicly released at https://yaoz720.github.io/SwarmAgentic/.", "title_zh": "《SwarmAgentic：基于群体智能的全自动化代理系统生成研究》", "AI": {"task": "提出SwarmAgentic框架以实现完全自动化的代理系统生成", "motivation": "现有代理系统生成框架缺乏完全自主性，限制了适应性和可扩展性", "method": "通过语言驱动的探索，从零开始构建代理系统，并通过反馈引导的更新进化候选系统群体", "result": "在六个现实世界的开放性和探索性任务中，SwarmAgentic在所有基线上表现优异，特别是在TravelPlanner基准上实现了+261.8%的相对改进", "conclusion": "SwarmAgentic框架标志着向可扩展和自主代理系统设计迈出了重要一步，将群体智能与完全自动化的系统多代理生成相结合"}}
{"id": "2506.14683v1", "pdf": "http://arxiv.org/pdf/2506.14683v1", "abs": "http://arxiv.org/abs/2506.14683v1", "authors": ["Leonhard Applis", "Yuntong Zhang", "Shanchao Liang", "Nan Jiang", "Lin Tan", "Abhik Roychoudhury"], "title": "Unified Software Engineering agent as AI Software Engineer", "categories": ["cs.SE", "cs.AI"], "comment": "Leonhard Applis and Yuntong Zhang contributed equally to this work", "summary": "The growth of Large Language Model (LLM) technology has raised expectations\nfor automated coding. However, software engineering is more than coding and is\nconcerned with activities including maintenance and evolution of a project. In\nthis context, the concept of LLM agents has gained traction, which utilize LLMs\nas reasoning engines to invoke external tools autonomously. But is an LLM agent\nthe same as an AI software engineer? In this paper, we seek to understand this\nquestion by developing a Unified Software Engineering agent or USEagent. Unlike\nexisting work which builds specialized agents for specific software tasks such\nas testing, debugging, and repair, our goal is to build a unified agent which\ncan orchestrate and handle multiple capabilities. This gives the agent the\npromise of handling complex scenarios in software development such as fixing an\nincomplete patch, adding new features, or taking over code written by others.\nWe envision USEagent as the first draft of a future AI Software Engineer which\ncan be a team member in future software development teams involving both AI and\nhumans. To evaluate the efficacy of USEagent, we build a Unified Software\nEngineering bench (USEbench) comprising of myriad tasks such as coding,\ntesting, and patching. USEbench is a judicious mixture of tasks from existing\nbenchmarks such as SWE-bench, SWT-bench, and REPOCOD. In an evaluation on\nUSEbench consisting of 1,271 repository-level software engineering tasks,\nUSEagent shows improved efficacy compared to existing general agents such as\nOpenHands CodeActAgent. There exist gaps in the capabilities of USEagent for\ncertain coding tasks, which provides hints on further developing the AI\nSoftware Engineer of the future.", "title_zh": "统一软件工程代理作为人工智能软件工程师", "AI": {"task": "理解LLM代理是否等同于AI软件工程师，并开发一个统一的软件工程代理（USEagent）", "motivation": "大型语言模型（LLM）技术的增长提高了对自动化编码的期望，但软件工程不仅仅是编码，还包括项目的维护和演进等活动", "method": "开发一个统一的软件工程代理（USEagent），能够协调和处理多种能力，而不是为特定的软件任务构建专门的代理", "result": "在包含1,271个仓库级软件工程任务的USEbench评估中，USEagent显示出比现有通用代理（如OpenHands CodeActAgent）更高的效能", "conclusion": "USEagent作为未来AI软件工程师的初稿，展示了在涉及AI和人类的未来软件开发团队中作为团队成员的潜力，尽管在某些编码任务上仍存在能力差距"}}
{"id": "2506.14670v1", "pdf": "http://arxiv.org/pdf/2506.14670v1", "abs": "http://arxiv.org/abs/2506.14670v1", "authors": ["Jina Kim", "Leeje Jang", "Yao-Yi Chiang", "Guanyu Wang", "Michelle Pasco"], "title": "StreetLens: Enabling Human-Centered AI Agents for Neighborhood Assessment from Street View Imagery", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Traditionally, neighborhood studies have employed interviews, surveys, and\nmanual image annotation guided by detailed protocols to identify environmental\ncharacteristics, including physical disorder, decay, street safety, and\nsociocultural symbols, and to examine their impact on developmental and health\noutcomes. While these methods yield rich insights, they are time-consuming and\nrequire intensive expert intervention. Recent technological advances, including\nvision-language models (VLMs), have begun to automate parts of this process;\nhowever, existing efforts are often ad hoc and lack adaptability across\nresearch designs and geographic contexts. In this demo paper, we present\nStreetLens, a human-centered, researcher-configurable workflow that embeds\nrelevant social science expertise in a VLM for scalable neighborhood\nenvironmental assessments. StreetLens mimics the process of trained human\ncoders by grounding the analysis in questions derived from established\ninterview protocols, retrieving relevant street view imagery (SVI), and\ngenerating a wide spectrum of semantic annotations from objective features\n(e.g., the number of cars) to subjective perceptions (e.g., the sense of\ndisorder in an image). By enabling researchers to define the VLM's role through\ndomain-informed prompting, StreetLens places domain knowledge at the core of\nthe analysis process. It also supports the integration of prior survey data to\nenhance robustness and expand the range of characteristics assessed across\ndiverse settings. We provide a Google Colab notebook to make StreetLens\naccessible and extensible for researchers working with public or custom SVI\ndatasets. StreetLens represents a shift toward flexible, agentic AI systems\nthat work closely with researchers to accelerate and scale neighborhood\nstudies.", "title_zh": "《StreetLens：基于街景图像实现以人为中心的社区评估AI智能体》", "AI": {"task": "开发一个可配置的工作流程StreetLens，用于自动化邻里环境评估", "motivation": "传统邻里研究方法耗时且需要专家密集干预，现有技术方法缺乏跨研究设计和地理环境的适应性", "method": "提出StreetLens，一个以人为中心、研究者可配置的工作流程，将社会科学专业知识嵌入视觉语言模型(VLM)中，用于可扩展的邻里环境评估", "result": "StreetLens通过从既定访谈协议中提取问题、检索相关街景图像(SVI)并生成从客观特征到主观感知的广泛语义注释，模仿了训练有素的人类编码员的过程", "conclusion": "StreetLens代表了向灵活、主动的AI系统的转变，这些系统与研究人员紧密合作，加速和扩展邻里研究"}}
{"id": "2506.14539v1", "pdf": "http://arxiv.org/pdf/2506.14539v1", "abs": "http://arxiv.org/abs/2506.14539v1", "authors": ["Daewon Kang", "YeongHwan Shin", "Doyeon Kim", "Kyu-Hwan Jung", "Meong Hi Son"], "title": "Doppelgänger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack", "categories": ["cs.AI", "cs.CR"], "comment": null, "summary": "Since the advent of large language models, prompt engineering now enables the\nrapid, low-effort creation of diverse autonomous agents that are already in\nwidespread use. Yet this convenience raises urgent concerns about the safety,\nrobustness, and behavioral consistency of the underlying prompts, along with\nthe pressing challenge of preventing those prompts from being exposed to user's\nattempts. In this paper, we propose the ''Doppelg\\\"anger method'' to\ndemonstrate the risk of an agent being hijacked, thereby exposing system\ninstructions and internal information. Next, we define the ''Prompt Alignment\nCollapse under Adversarial Transfer (PACAT)'' level to evaluate the\nvulnerability to this adversarial transfer attack. We also propose a ''Caution\nfor Adversarial Transfer (CAT)'' prompt to counter the Doppelg\\\"anger method.\nThe experimental results demonstrate that the Doppelg\\\"anger method can\ncompromise the agent's consistency and expose its internal information. In\ncontrast, CAT prompts enable effective defense against this adversarial attack.", "title_zh": "替身攻击法：基于提示的可迁移对抗攻击打破大语言模型智能体的角色一致性", "AI": {"task": "评估大型语言模型在提示工程下的安全性、鲁棒性和行为一致性", "motivation": "提示工程的便利性引发了关于底层提示的安全性、鲁棒性和行为一致性的紧迫担忧，以及防止这些提示暴露于用户尝试的挑战", "method": "提出'Doppelgänger方法'来展示代理被劫持的风险，定义'Prompt Alignment Collapse under Adversarial Transfer (PACAT)'级别来评估对这种对抗性转移攻击的脆弱性，并提出'Caution for Adversarial Transfer (CAT)'提示来对抗Doppelgänger方法", "result": "实验结果表明，Doppelgänger方法可以破坏代理的一致性并暴露其内部信息，而CAT提示能够有效防御这种对抗性攻击", "conclusion": "CAT提示是防御对抗性转移攻击的有效方法"}}
{"id": "2506.14142v1", "pdf": "http://arxiv.org/pdf/2506.14142v1", "abs": "http://arxiv.org/abs/2506.14142v1", "authors": ["Wenting Chen", "Yi Dong", "Zhaojun Ding", "Yucheng Shi", "Yifan Zhou", "Fang Zeng", "Yijun Luo", "Tianyu Lin", "Yihang Su", "Yichen Wu", "Kai Zhang", "Zhen Xiang", "Tianming Liu", "Ninghao Liu", "Lichao Sun", "Yixuan Yuan", "Xiang Li"], "title": "RadFabric: Agentic AI System with Reasoning Capability for Radiology", "categories": ["cs.CV", "cs.CL"], "comment": "4 figures, 2 tables", "summary": "Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic\nconditions, but current automated systems face limitations in pathology\ncoverage, diagnostic accuracy, and integration of visual and textual reasoning.\nTo address these gaps, we propose RadFabric, a multi agent, multimodal\nreasoning framework that unifies visual and textual analysis for comprehensive\nCXR interpretation. RadFabric is built on the Model Context Protocol (MCP),\nenabling modularity, interoperability, and scalability for seamless integration\nof new diagnostic agents. The system employs specialized CXR agents for\npathology detection, an Anatomical Interpretation Agent to map visual findings\nto precise anatomical structures, and a Reasoning Agent powered by large\nmultimodal reasoning models to synthesize visual, anatomical, and clinical data\ninto transparent and evidence based diagnoses. RadFabric achieves significant\nperformance improvements, with near-perfect detection of challenging\npathologies like fractures (1.000 accuracy) and superior overall diagnostic\naccuracy (0.799) compared to traditional systems (0.229 to 0.527). By\nintegrating cross modal feature alignment and preference-driven reasoning,\nRadFabric advances AI-driven radiology toward transparent, anatomically\nprecise, and clinically actionable CXR analysis.", "title_zh": "RadFabric：具备放射学推理能力的智能代理AI系统", "AI": {"task": "提升胸部X光（CXR）成像在胸部疾病诊断中的自动化系统的病理覆盖范围、诊断准确性以及视觉和文本推理的整合能力", "motivation": "当前自动化系统在病理覆盖范围、诊断准确性和视觉与文本推理整合方面存在局限", "method": "提出RadFabric，一个多代理、多模态推理框架，基于模型上下文协议（MCP）构建，实现模块化、互操作性和可扩展性，整合新的诊断代理", "result": "RadFabric在挑战性病理如骨折的检测上达到近乎完美的准确度（1.000），整体诊断准确性（0.799）显著优于传统系统（0.229至0.527）", "conclusion": "通过整合跨模态特征对齐和偏好驱动的推理，RadFabric推动了AI驱动的放射学向透明、解剖学精确和临床可操作的CXR分析迈进"}}
{"id": "2506.13905v1", "pdf": "http://arxiv.org/pdf/2506.13905v1", "abs": "http://arxiv.org/abs/2506.13905v1", "authors": ["Zhongzhi Yu", "Mingjie Liu", "Michael Zimmer", "Yingyan Celine Lin", "Yong Liu", "Haoxing Ren"], "title": "Spec2RTL-Agent: Automated Hardware Code Generation from Complex Specifications Using LLM Agent Systems", "categories": ["cs.AR"], "comment": null, "summary": "Despite recent progress in generating hardware RTL code with LLMs, existing\nsolutions still suffer from a substantial gap between practical application\nscenarios and the requirements of real-world RTL code development. Prior\napproaches either focus on overly simplified hardware descriptions or depend on\nextensive human guidance to process complex specifications, limiting their\nscalability and automation potential. In this paper, we address this gap by\nproposing an LLM agent system, termed Spec2RTL-Agent, designed to directly\nprocess complex specification documentation and generate corresponding RTL code\nimplementations, advancing LLM-based RTL code generation toward more realistic\napplication settings. To achieve this goal, Spec2RTL-Agent introduces a novel\nmulti-agent collaboration framework that integrates three key enablers: (1) a\nreasoning and understanding module that translates specifications into\nstructured, step-by-step implementation plans; (2) a progressive coding and\nprompt optimization module that iteratively refines the code across multiple\nrepresentations to enhance correctness and synthesisability for RTL conversion;\nand (3) an adaptive reflection module that identifies and traces the source of\nerrors during generation, ensuring a more robust code generation flow. Instead\nof directly generating RTL from natural language, our system strategically\ngenerates synthesizable C++ code, which is then optimized for HLS. This\nagent-driven refinement ensures greater correctness and compatibility compared\nto naive direct RTL generation approaches. We evaluate Spec2RTL-Agent on three\nspecification documents, showing it generates accurate RTL code with up to 75%\nfewer human interventions than existing methods. This highlights its role as\nthe first fully automated multi-agent system for RTL generation from\nunstructured specs, reducing reliance on human effort in hardware design.", "title_zh": "Spec2RTL-Agent：基于LLM智能体系统的复杂硬件规范自动代码生成", "AI": {"task": "提出Spec2RTL-Agent系统，旨在直接从复杂的规范文档生成对应的RTL代码实现，推动基于LLM的RTL代码生成向更现实的应用场景迈进", "motivation": "现有的解决方案在实际应用场景和真实世界RTL代码开发需求之间存在显著差距，限制了其可扩展性和自动化潜力", "method": "引入一个新颖的多智能体协作框架，集成了三个关键促成因素：推理和理解模块、渐进式编码和提示优化模块、自适应反射模块", "result": "在三个规范文档上评估Spec2RTL-Agent，显示其生成的RTL代码比现有方法减少了高达75%的人工干预", "conclusion": "Spec2RTL-Agent作为第一个从非结构化规范完全自动化的多智能体系统，减少了硬件设计中的人力依赖"}}
{"id": "2506.13474v1", "pdf": "http://arxiv.org/pdf/2506.13474v1", "abs": "http://arxiv.org/abs/2506.13474v1", "authors": ["David Bani-Harouni", "Chantal Pellegrini", "Ege Özsoy", "Matthias Keicher", "Nassir Navab"], "title": "Language Agents for Hypothesis-driven Clinical Decision Making with Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Clinical decision-making is a dynamic, interactive, and cyclic process where\ndoctors have to repeatedly decide on which clinical action to perform and\nconsider newly uncovered information for diagnosis and treatment. Large\nLanguage Models (LLMs) have the potential to support clinicians in this\nprocess, however, most applications of LLMs in clinical decision support suffer\nfrom one of two limitations: Either they assume the unrealistic scenario of\nimmediate availability of all patient information and do not model the\ninteractive and iterative investigation process, or they restrict themselves to\nthe limited \"out-of-the-box\" capabilities of large pre-trained models without\nperforming task-specific training. In contrast to this, we propose to model\nclinical decision-making for diagnosis with a hypothesis-driven\nuncertainty-aware language agent, LA-CDM, that converges towards a diagnosis\nvia repeatedly requesting and interpreting relevant tests. Using a hybrid\ntraining paradigm combining supervised and reinforcement learning, we train\nLA-CDM with three objectives targeting critical aspects of clinical\ndecision-making: accurate hypothesis generation, hypothesis uncertainty\nestimation, and efficient decision-making. We evaluate our methodology on\nMIMIC-CDM, a real-world dataset covering four abdominal diseases containing\nvarious clinical tests and show the benefit of explicitly training clinical\ndecision-making for increasing diagnostic performance and efficiency.", "title_zh": "基于强化学习的假设驱动临床决策语言智能体", "AI": {"task": "建模临床决策过程以支持诊断", "motivation": "大型语言模型在临床决策支持中的应用存在两种局限：要么假设所有患者信息立即可用，不模拟交互式和迭代的调查过程；要么仅限于大型预训练模型的'开箱即用'能力，不进行任务特定训练", "method": "提出一个假设驱动的不确定性感知语言代理LA-CDM，通过反复请求和解释相关测试来收敛于诊断，采用结合监督学习和强化学习的混合训练范式，针对临床决策制定的三个关键方面进行训练：准确假设生成、假设不确定性估计和高效决策制定", "result": "在MIMIC-CDM真实世界数据集上评估方法，该数据集涵盖四种腹部疾病和各种临床测试，显示明确训练临床决策制定对提高诊断性能和效率的益处", "conclusion": "假设驱动的不确定性感知语言代理LA-CDM能有效支持临床决策过程，提高诊断性能和效率"}}
{"id": "2506.13171v1", "pdf": "http://arxiv.org/pdf/2506.13171v1", "abs": "http://arxiv.org/abs/2506.13171v1", "authors": ["Lukasz Mazur", "Nenad Petrovic", "James Pontes Miranda", "Ansgar Radermacher", "Robert Rasche", "Alois Knoll"], "title": "Querying Large Automotive Software Models: Agentic vs. Direct LLM Approaches", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) offer new opportunities for interacting with\ncomplex software artifacts, such as software models, through natural language.\nThey present especially promising benefits for large software models that are\ndifficult to grasp in their entirety, making traditional interaction and\nanalysis approaches challenging. This paper investigates two approaches for\nleveraging LLMs to answer questions over software models: direct prompting,\nwhere the whole software model is provided in the context, and an agentic\napproach combining LLM-based agents with general-purpose file access tools. We\nevaluate these approaches using an Ecore metamodel designed for timing analysis\nand software optimization in automotive and embedded domains. Our findings show\nthat while the agentic approach achieves accuracy comparable to direct\nprompting, it is significantly more efficient in terms of token usage. This\nefficiency makes the agentic approach particularly suitable for the automotive\nindustry, where the large size of software models makes direct prompting\ninfeasible, establishing LLM agents as not just a practical alternative but the\nonly viable solution. Notably, the evaluation was conducted using small LLMs,\nwhich are more feasible to be executed locally - an essential advantage for\nmeeting strict requirements around privacy, intellectual property protection,\nand regulatory compliance. Future work will investigate software models in\ndiverse formats, explore more complex agent architectures, and extend agentic\nworkflows to support not only querying but also modification of software\nmodels.", "title_zh": "查询大型汽车软件模型：智能代理与直接LLM方法对比", "AI": {"task": "利用大型语言模型（LLMs）回答关于软件模型的问题", "motivation": "大型软件模型难以整体把握，传统交互和分析方法面临挑战", "method": "研究两种方法：直接提示法和基于LLM的代理与通用文件访问工具结合的代理法", "result": "代理法在准确性上与直接提示法相当，但在令牌使用上显著更高效", "conclusion": "代理法特别适合汽车行业，对于大型软件模型，LLM代理不仅是实用的替代方案，而且是唯一可行的解决方案"}}
{"id": "2506.12666v1", "pdf": "http://arxiv.org/pdf/2506.12666v1", "abs": "http://arxiv.org/abs/2506.12666v1", "authors": ["Hitesh Goel", "Hao Zhu"], "title": "LIFELONG SOTOPIA: Evaluating Social Intelligence of Language Agents Over Lifelong Social Interactions", "categories": ["cs.AI"], "comment": null, "summary": "Humans engage in lifelong social interactions through interacting with\ndifferent people under different scenarios for different social goals. This\nrequires social intelligence to gather information through a long time span and\nuse it to navigate various social contexts effectively. Whether AI systems are\nalso capable of this is understudied in the existing research. In this paper,\nwe present a novel benchmark, LIFELONG-SOTOPIA, to perform a comprehensive\nevaluation of language agents by simulating multi-episode interactions. In each\nepisode, the language agents role-play characters to achieve their respective\nsocial goals in randomly sampled social tasks. With LIFELONG-SOTOPIA, we find\nthat goal achievement and believability of all of the language models that we\ntest decline through the whole interaction. Although using an advanced memory\nmethod improves the agents' performance, the best agents still achieve a\nsignificantly lower goal completion rate than humans on scenarios requiring an\nexplicit understanding of interaction history. These findings show that we can\nuse LIFELONG-SOTOPIA to evaluate the social intelligence of language agents\nover lifelong social interactions.", "title_zh": "终身社交智能评估：语言代理在长期社会互动中的表现研究", "AI": {"task": "评估语言代理在终身社交互动中的社交智能", "motivation": "现有研究未充分探讨AI系统是否能够像人类一样通过长期社交互动收集信息并有效导航各种社交情境", "method": "提出新基准LIFELONG-SOTOPIA，通过模拟多集互动对语言代理进行全面评估", "result": "发现所有测试的语言模型的目标达成度和可信度在整个互动过程中下降，尽管使用先进记忆方法提高了代理性能，但在需要明确理解互动历史的情境中，最佳代理的目标完成率仍显著低于人类", "conclusion": "LIFELONG-SOTOPIA可用于评估语言代理在终身社交互动中的社交智能"}}
{"id": "2506.12636v1", "pdf": "http://arxiv.org/pdf/2506.12636v1", "abs": "http://arxiv.org/abs/2506.12636v1", "authors": ["Julia Santaniello", "Matthew Russell", "Benson Jiang", "Donatello Sassaroli", "Robert Jacob", "Jivko Sinapov"], "title": "Mapping Neural Signals to Agent Performance, A Step Towards Reinforcement Learning from Neural Feedback", "categories": ["cs.LG"], "comment": null, "summary": "Implicit Human-in-the-Loop Reinforcement Learning (HITL-RL) is a methodology\nthat integrates passive human feedback into autonomous agent training while\nminimizing human workload. However, existing methods often rely on active\ninstruction, requiring participants to teach an agent through unnatural\nexpression or gesture. We introduce NEURO-LOOP, an implicit feedback framework\nthat utilizes the intrinsic human reward system to drive human-agent\ninteraction. This work demonstrates the feasibility of a critical first step in\nthe NEURO-LOOP framework: mapping brain signals to agent performance. Using\nfunctional near-infrared spectroscopy (fNIRS), we design a dataset to enable\nfuture research using passive Brain-Computer Interfaces for Human-in-the-Loop\nReinforcement Learning. Participants are instructed to observe or guide a\nreinforcement learning agent in its environment while signals from the\nprefrontal cortex are collected. We conclude that a relationship between fNIRS\ndata and agent performance exists using classical machine learning techniques.\nFinally, we highlight the potential that neural interfaces may offer to future\napplications of human-agent interaction, assistive AI, and adaptive autonomous\nsystems.", "title_zh": "《将神经信号映射至智能体表现：迈向基于神经反馈的强化学习》", "AI": {"task": "将被动人类反馈整合到自主代理训练中的隐式人机交互强化学习（HITL-RL）方法", "motivation": "现有方法通常依赖主动指令，要求参与者通过不自然的表达或手势教导代理", "method": "引入NEURO-LOOP框架，利用内在人类奖励系统驱动人机交互，并使用功能性近红外光谱（fNIRS）设计数据集", "result": "使用经典机器学习技术证实了fNIRS数据与代理性能之间存在关系", "conclusion": "神经接口可能为未来的人机交互、辅助AI和自适应自主系统应用提供潜力"}}
